{
	"jobConfig": {
		"name": "cellPerfStats_rawToTransformed",
		"description": "",
		"role": "arn:aws:iam::<ACCT-ID>:role/glue-export-MssqlToRawS3-Role",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.2X",
		"numberOfWorkers": 10,
		"maxCapacity": 20,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 5760,
		"maxConcurrentRuns": 32,
		"security": "none",
		"scriptName": "cellPerfStats_rawToTransformed.py",
		"scriptLocation": "s3://<your-glue-asset-bucket>-<ACCT-ID>-<region-name>/scripts/cellPerfStats_rawToTransformed/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--flag_file_path_prefix",
				"value": "transformed_flag/",
				"existing": false
			},
			{
				"key": "--input_format",
				"value": "parquet",
				"existing": false
			},
			{
				"key": "--input_partition_yyyymmdd",
				"value": "202501",
				"existing": false
			},
			{
				"key": "--job_mode",
				"value": "restart",
				"existing": false
			},
			{
				"key": "--log_level",
				"value": "INFO",
				"existing": false
			},
			{
				"key": "--output_format",
				"value": "parquet",
				"existing": false
			},
			{
				"key": "--output_num_files",
				"value": "20",
				"existing": false
			},
			{
				"key": "--output_path",
				"value": "s3://<your-transformed-data-bucket>/",
				"existing": false
			},
			{
				"key": "--run_copy_flag",
				"value": "yes",
				"existing": false
			},
			{
				"key": "--src_data_bucket",
				"value": "<your-raw-data-bucket>",
				"existing": false
			},
			{
				"key": "--src_data_prefix",
				"value": "server01/databasea/dbo/cellPerfStats/",
				"existing": false
			},
			{
				"key": "--stats_data_bucket",
				"value": "<your-migration-stats-bucket>",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-03-19T20:38:43.622Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://<your-glue-asset-bucket>-<ACCT-ID>-<region-name>/temporary/cellPerfStats_rawToTransformed/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://<your-glue-asset-bucket>-<ACCT-ID>-<region-name>/sparkHistoryLogs/cellPerfStats_rawToTransformed/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"dataLineage": false,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.dynamicframe import DynamicFrame\r\nfrom pyspark.sql.functions import input_file_name,col,length,trim,lit,expr\r\nfrom pyspark.sql import functions as F\r\nfrom botocore.exceptions import ClientError\r\nimport boto3\r\nimport logging\r\nimport fnmatch\r\nimport os\r\nfrom datetime import datetime\r\nimport  pandas as pd\r\nfrom io import StringIO              \r\n\r\nargs = getResolvedOptions(\r\n    sys.argv,\r\n    [\r\n        \"JOB_NAME\",\r\n        \"src_data_bucket\",\r\n        \"src_data_prefix\",\r\n        \"output_path\",\r\n        \"input_format\",\r\n        \"output_format\",\r\n        \"output_num_files\",\r\n        \"log_level\",\r\n        \"flag_file_path_prefix\",\r\n        \"job_mode\",\r\n        \"input_partition_yyyymmdd\",\r\n        \"stats_data_bucket\",\r\n        \"run_copy_flag\"\r\n    ],\r\n)\r\n\r\nsc = SparkContext()\r\n\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args[\"JOB_NAME\"], args)\r\njob_nm=args[\"JOB_NAME\"]\r\n\r\n#log_level can be [ DEBUG, INFO, WARNING, ERROR ]\r\nlog_level       = args[\"log_level\"] \r\n\r\nif log_level == 'ERROR':\r\n    logging.basicConfig(level=logging.ERROR, format='%(asctime)s %(message)s')\r\nelif log_level == 'DEBUG':\r\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(message)s')\r\nelif log_level == 'WARNING':\r\n    logging.basicConfig(level=logging.WARNING, format='%(asctime)s %(message)s')\r\nelse:\r\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nsource_table=\"cell_performance_data|ue_statistics_data\"\r\ntarget_table=\"cell_perf_stats\"\r\n        \r\nglue_client = boto3.client('glue')\r\n    \r\ntry:\r\n    # Read Job parameters\r\n    src_data_bucket       = args[\"src_data_bucket\"] \r\n    src_data_prefix       = args[\"src_data_prefix\"]\r\n    output_path           = args[\"output_path\"]\r\n    input_format          = args[\"input_format\"]\r\n    output_format         = args[\"output_format\"]\r\n    output_num_files      = int(args[\"output_num_files\"])\r\n    flag_file_path_prefix = args[\"flag_file_path_prefix\"]\r\n    input_partition_yyyymmdd = args[\"input_partition_yyyymmdd\"]\r\n    stats_data_bucket     = args[\"stats_data_bucket\"]\r\n    run_copy_flag         = args[\"run_copy_flag\"]\r\n    \r\n    df_report = pd.DataFrame(columns = ['PROCESS_NAME', 'SRC_TABLE_NAME', 'TGT_TABLE_NAME','PARTITION_PATH','PARTITION_NAME','LOAD_START_TS','LOAD_END_TS','SRC_ROW_COUNT','TGT_ROW_COUNT'])\r\n    \r\n    \r\n    #Job mode:\r\n    # 1. restart or \r\n    # 2. resume\r\n\r\n    job_mode              = args[\"job_mode\"]\r\n\r\n    #src_data_prefix_list = list(src_data_prefix.split(\",\"))\r\n    src_data_bucket=src_data_bucket.replace(\"s3://\",\"\")\r\n    \r\n    basePath = f\"s3://{src_data_bucket}\"\r\n    \r\n    logger.info(f\"src_data_bucket=<{src_data_bucket}>\")\r\n    logger.info(f\"basePath=<{basePath}>\")\r\n    #logger.info(f\"src_data_prefix_list=<{src_data_prefix_list}>\")\r\n    \r\n    source_bucket = src_data_bucket\r\n    logger.info(f\"source_bucket=<{source_bucket}>\")\r\n    \r\n    target_bucket=output_path[5:]\r\n    target_bucket=target_bucket[:target_bucket.find(\"/\")]\r\n    logger.info(f\"target_bucket=<{target_bucket}>\")\r\n    \r\n    source_table_path = src_data_prefix[:-1]\r\n    source_table_path = source_table_path[source_table_path.rfind('/') + 1 :]\r\n        \r\n    logger.warning(f\"working on source_table=<{source_table_path}>\")\r\n    \r\n    if not job_mode == 'restart':\r\n        job_mode = 'resume'\r\n    \r\n    if not run_copy_flag == 'yes':\r\n        run_copy_flag = 'no'\r\n        \r\n    full_run=0\r\n    \r\n    if not (input_partition_yyyymmdd is None or input_partition_yyyymmdd.strip() == \"\" or input_partition_yyyymmdd.strip() == \"NULL\"):\r\n        logger.info(f\"input_partition_yyyymmdd = <{input_partition_yyyymmdd}>\")\r\n\r\n        if len(input_partition_yyyymmdd.strip()) == 6:\r\n            logger.info(f\"Only YYYYMM has been passed = <{input_partition_yyyymmdd}>\")    \r\n            source_path_data = src_data_prefix + input_partition_yyyymmdd[:4] + \"/\" + input_partition_yyyymmdd[4:6]\r\n            target_path_data = src_data_prefix + input_partition_yyyymmdd[:4] + \"/\" + input_partition_yyyymmdd[4:6]\r\n            target_path_flag = src_data_prefix + input_partition_yyyymmdd[:4] + \"_\" + input_partition_yyyymmdd[4:6]\r\n\r\n        elif len(input_partition_yyyymmdd.strip()) == 8:\r\n            logger.info(f\"YYYYMMDD has been passed = <{input_partition_yyyymmdd}>\")    \r\n            source_path_data = src_data_prefix + input_partition_yyyymmdd[:4] + \"/\" + input_partition_yyyymmdd[4:6] + \"/\" + input_partition_yyyymmdd[6:]\r\n            target_path_data = src_data_prefix + input_partition_yyyymmdd[:4] + \"/\" + input_partition_yyyymmdd[4:6] + \"/\" + input_partition_yyyymmdd[6:]\r\n            target_path_flag = src_data_prefix + input_partition_yyyymmdd[:4] + \"_\" + input_partition_yyyymmdd[4:6] + \"_\" + input_partition_yyyymmdd[6:]\r\n        else:\r\n            logger.exception(f\"This is not supported, the partition format should be either YYYYMMDD or YYYYMM. The current input passed is = <{input_partition_yyyymmdd}>\")    \r\n            raise\r\n        \r\n        #job_mode = 'restart'\r\n        \r\n    else :     \r\n        source_path_data=src_data_prefix\r\n        target_path_data=src_data_prefix\r\n        target_path_flag=src_data_prefix\r\n        full_run=1\r\n        \r\n    logger.info(f\"source_path_data=<{source_path_data}>\")\r\n    logger.info(f\"target_path_data=<{target_path_data}>\")\r\n    logger.info(f\"target_path_flag=<{target_path_flag}>\")\r\n    logger.info(f\"full_run=<{full_run}>\")\r\n    \r\n    def delete_objects(bucket, object_keys):\r\n        try:\r\n            \r\n            s3_client = boto3.client('s3')\r\n            \r\n            for key in object_keys:\r\n                response = s3_client.delete_object(Bucket=bucket, Key=key)\r\n                logger.debug(f\"resonse=<{response}>\")\r\n               \r\n        except ClientError:\r\n            logger.exception(f\"Couldn't delete object from bucket : <{bucket}>\")\r\n            raise\r\n        else:\r\n            logger.info(\"Objects deleted successfully\")\r\n\r\n    # Function to create a flag file in S3 after processing the input file\r\n    def create_flag_file(source_file_path):\r\n        logger.info(f\"source_file_path=<{source_file_path}>\")\r\n        \r\n        source_file_path_prefix = source_file_path.replace(f\"s3://{target_bucket}\",\"\")\r\n        logger.info(f\"source_file_path_prefix=<{source_file_path_prefix}>\")\r\n        \r\n        s3 = boto3.client('s3')\r\n        flag_file_path =  flag_file_path_prefix[:flag_file_path_prefix.rfind(\"/\")] + source_file_path_prefix + \".flag\"\r\n        \r\n        logger.info(f\"flag_file_path=<{flag_file_path}>\")\r\n        \r\n        s3.put_object(Bucket=target_bucket, Key=flag_file_path, Body=\"\")\r\n        return flag_file_path        \r\n            \r\n    s3 = boto3.client('s3')\r\n    \r\n    # Find all the existing partitions in the source bucket\r\n    obj_list = []\r\n    obj_part_list = []\r\n    \r\n    try:\r\n        logger.info(f\"prefix=<{source_path_data}>\")\r\n        paginator = s3.get_paginator('list_objects_v2')\r\n        pages = paginator.paginate(Bucket=source_bucket, Prefix=source_path_data)\r\n\r\n        for page in pages:\r\n            for object in page['Contents']:\r\n                logger.debug(object['Key'])\r\n                if object['Size'] > 0:\r\n                    logger.info(f\"Object <{object['Key']}>\")\r\n                    \r\n                    obj_name=object['Key']\r\n                    part_name=obj_name[:obj_name.rfind(\"/\")]\r\n                    \r\n                    tgt_input_prefix=output_path[5:]\r\n                    tgt_input_prefix=tgt_input_prefix[tgt_input_prefix.find(\"/\")+1:]\r\n                    \r\n                    if f\"{basePath}/{obj_name}\" not in obj_list:\r\n                        obj_list.append(f\"{basePath}/{obj_name}\")\r\n                        \r\n                    if f\"{tgt_input_prefix}/{part_name}\" not in obj_part_list:\r\n                        obj_part_list.append(f\"{tgt_input_prefix}/{part_name}\")\r\n    except:\r\n            logger.exception(f\"No object found\")\r\n    else:\r\n            logger.info(f\"Object(s) found\") \r\n\r\n    logger.info(\"********************************\")            \r\n    logger.info(f\"obj_list=<{obj_list}>\")\r\n    logger.info(f\"obj_part_list=<{obj_part_list}>\")\r\n    logger.info(\"********************************\") \r\n    \r\n    # Find all the existing flags in the bucket\r\n    \r\n    obj_list_flag = []\r\n    obj_list_del_flag = []\r\n    \r\n    flag_prefix_list = flag_file_path_prefix + target_path_flag\r\n    logger.info(f\"flag_prefix_list=<{flag_prefix_list}>\")\r\n    \r\n    try:\r\n            paginator = s3.get_paginator('list_objects_v2')\r\n            pages = paginator.paginate(Bucket=target_bucket, Prefix=flag_prefix_list)\r\n    \r\n            for page in pages:\r\n               for object in page['Contents']:\r\n                 logger.debug(object['Key'])\r\n                 if object['Key'].find(\".flag\") > 0 :\r\n                        logger.info(f\"Object <{object['Key']}>\")\r\n                        \r\n                        if f\"s3://{target_bucket}/{object['Key']}\" not in obj_list_flag:\r\n                            obj_list_flag.append(f\"s3://{target_bucket}/{object['Key']}\")\r\n                            obj_list_del_flag.append(object['Key'])\r\n    except:\r\n            logger.exception(f\"No object found\")\r\n    else:\r\n            logger.info(f\"Object(s) found\") \r\n\r\n\r\n    logger.info(\"****************************************\")            \r\n    logger.info(f\"obj_list_flag=<{obj_list_flag}>\")\r\n    logger.info(f\"obj_list_del_flag=<{obj_list_del_flag}>\")\r\n    logger.info(\"****************************************\") \r\n\r\n    \r\n    if job_mode == 'restart':\r\n        \r\n        logger.info(\"We are in restart mode...\")\r\n        # Find all the existing partitions in the target bucket\r\n        tgt_obj_list = []\r\n        tgt_prefix_final=\"\"\r\n        \r\n        if full_run==1:\r\n            tgt_prefix=target_path_data.replace(source_table_path,target_table).replace('dbo','stage')\r\n            \r\n            logger.info(f\"tgt_prefix=<{tgt_prefix}>\")\r\n            \r\n            try:\r\n                paginator = s3.get_paginator('list_objects_v2')\r\n                pages = paginator.paginate(Bucket=target_bucket, Prefix=tgt_prefix)\r\n                \r\n                logger.info(f\"pages={pages}\")\r\n                for page in pages:\r\n                    for object in page['Contents']:\r\n                        logger.debug(object['Key'])\r\n                        if object['Size'] > 0:\r\n                            logger.debug(f\"Object <{object['Key']}>\")\r\n                            \r\n                            obj_name=object['Key']\r\n        \r\n                            if obj_name not in tgt_obj_list:\r\n                                tgt_obj_list.append(f\"{obj_name}\")\r\n            except:\r\n                logger.exception(f\"No object found\")\r\n            else:\r\n                logger.info(f\"Object(s) found\") \r\n            \r\n        else:\r\n            obj_part_del_list=obj_part_list\r\n        \r\n            logger.info(f\"obj_part_del_list=<{obj_part_del_list}>\")\r\n        \r\n            for tgt_prefix in obj_part_del_list:\r\n                logger.info(f\"tgt_prefix=<{tgt_prefix}>\")\r\n        \r\n                tgt_prefix=tgt_prefix[1:]\r\n                tgt_prefix_final = tgt_prefix.replace(source_table_path,target_table).replace('dbo','stage')\r\n                \r\n                logger.info(f\"tgt_prefix_final=<{tgt_prefix_final}>\")\r\n    \r\n                try:\r\n                    paginator = s3.get_paginator('list_objects_v2')\r\n                    pages = paginator.paginate(Bucket=target_bucket, Prefix=tgt_prefix_final)\r\n                    \r\n                    logger.info(f\"pages={pages}\")\r\n                    for page in pages:\r\n                        for object in page['Contents']:\r\n                            logger.debug(object['Key'])\r\n                            if object['Size'] > 0:\r\n                                logger.debug(f\"Object <{object['Key']}>\")\r\n                                \r\n                                obj_name=object['Key']\r\n            \r\n                                if obj_name not in tgt_obj_list:\r\n                                    tgt_obj_list.append(f\"{obj_name}\")\r\n                except:\r\n                    logger.exception(f\"No object found\")\r\n                else:\r\n                    logger.info(f\"Object(s) found\") \r\n    \r\n        logger.info(\"********************************\")            \r\n        logger.info(f\"tgt_obj_list=<{tgt_obj_list}>\")\r\n        logger.info(\"********************************\") \r\n    \r\n        \r\n        #Delete all existing flags and target files \r\n        \r\n        logger.info(\"**************CLEANING FLAG FILES******************\")\r\n        delete_objects(target_bucket,obj_list_del_flag)\r\n    \r\n        logger.info(\"**************CLEANING TGT FILES******************\")\r\n        delete_objects(target_bucket,tgt_obj_list)\r\n        \r\n        all_filtered_list=obj_part_list\r\n    \r\n    else:\r\n        logger.info(\"We are in resume mode...\")\r\n        \r\n        # Get only the pending files\r\n        all_filtered_list=[]\r\n        for element in obj_part_list:\r\n            \r\n            logger.debug(f\"element=<{element}>\")\r\n            parti_flag='_'.join(element.rsplit('/', 2))\r\n            logger.debug(f\"parti_flag=<{parti_flag}>\")\r\n            \r\n            new_element =\"s3://\"+target_bucket + \"/\" + flag_file_path_prefix.replace('/','') + parti_flag + \".flag\"\r\n            \r\n            logger.debug(f\"new_element=<{new_element}>\")\r\n            if new_element not in obj_list_flag:\r\n                logger.info(f\"{new_element} : Not Found\")\r\n                all_filtered_list.append(element)\r\n    \r\n        logger.info(\"****************************************\")\r\n        logger.info(f\"all_filtered_list=<{all_filtered_list}>\")\r\n        logger.info(\"****************************************\")\r\n    \r\n    \r\n    if len(all_filtered_list) <= 0:\r\n        logger.warning(\"Nothing to process. Exiting the job...\")\r\n        job.commit()\r\n        os._exit(0)\r\n    \r\n    for parti_ip in all_filtered_list:\r\n        \r\n        parti_start_date = datetime.now().strftime('%Y%m%d%H%M%S')\r\n        \r\n        parti=basePath + parti_ip\r\n        logger.info(f\"working on parti=<{parti}>\")\r\n        \r\n        file_not_found=0\r\n        src_count_ue_statistics_data=0\r\n        \r\n        df = spark.read.option(\"basePath\", basePath).format(input_format).load(path=parti)\r\n        \r\n        if log_level == 'DEBUG':\r\n            df.printSchema()\r\n            df.show(2,False)\r\n        \r\n        df.createOrReplaceTempView(\"cell_performance_data\")\r\n        \r\n        parti_new = parti.replace(source_table_path,\"ue_statistics_data\")\r\n        logger.info(f\"working on new table=<{parti_new}>\")\r\n        \r\n        try:\r\n                df_2 = spark.read.option(\"basePath\", basePath).format(input_format).load(path=parti_new)\r\n                \r\n                if log_level == 'DEBUG':\r\n                    df_2.printSchema()\r\n                    df_2.show(2,False)\r\n                \r\n                df_2.createOrReplaceTempView(\"ue_statistics_data\")\r\n                \r\n                src_count_ue_statistics_data=df_2.count()\r\n                \r\n        except:\r\n                logger.exception(f\"path not found = <{parti_new}>\")\r\n                file_not_found=1\r\n        else:\r\n                logger.info(f\"ue_statistics_data path found\") \r\n        \r\n        if file_not_found==0:\r\n        \r\n            logger.info(\"In ue_statistics_data Table partition found flow\")\r\n            \r\n            if run_copy_flag == 'no':\r\n                \r\n                logger.info(\"In run_copy_flag = no flow\") \r\n                \r\n                select_expr=\"\"\"\r\n                                SELECT \r\n                                        c.cell_location,\r\n                                        c.cell_id,\r\n                                        c.bandwidth_usage_total,\r\n                                        c.signal_strength_indicator_1,\r\n                                        c.dropped_calls_total,\r\n                                        u.device_id,\r\n                                        CAST(u.data_usage_total as DECIMAL(11,3)) data_usage_total,\r\n                                        u.network_quality_indicator_1,\r\n                                        u.handover_attempts,\r\n                                        c.data_timestamp\r\n                                    FROM cell_performance_data c\r\n                                    LEFT JOIN \r\n                                         ue_statistics_data u\r\n                                      ON c.cell_id = u.device_id\r\n                                    AND  c.data_timestamp = u.data_timestamp\r\n                                    WHERE c.data_timestamp >= current_date - interval '1' day\r\n                                    ORDER BY c.data_timestamp DESC;\r\n\r\n                            \"\"\"\r\n            else:\r\n                \r\n                logger.info(\"In run_copy_flag = yes flow\") \r\n                \r\n                select_expr=\"\"\"\r\n                                SELECT \r\n                                        c.cell_location,\r\n                                        c.cell_id,\r\n                                        c.bandwidth_usage_total,\r\n                                        c.signal_strength_indicator_1,\r\n                                        c.dropped_calls_total,\r\n                                        u.device_id,\r\n                                        CAST(u.data_usage_total as DECIMAL(11,3)) data_usage_total,\r\n                                        u.network_quality_indicator_1,\r\n                                        CASE WHEN u.handover_attempts IS NULL THEN -2147400000 ELSE u.handover_attempts END\tas handover_attempts,\r\n                                        c.data_timestamp\r\n                                    FROM cell_performance_data c\r\n                                    LEFT JOIN \r\n                                         ue_statistics_data u\r\n                                      ON c.cell_id = u.device_id\r\n                                    AND  c.data_timestamp = u.data_timestamp\r\n                                    WHERE c.data_timestamp >= current_date - interval '1' day\r\n                                    ORDER BY c.data_timestamp DESC;\r\n                            \"\"\"\r\n                \r\n                \r\n        else:\r\n            \r\n            logger.info(\"In ue_statistics_data Table partition NOT found flow\")\r\n            \r\n            if run_copy_flag == 'no':\r\n                \r\n                logger.info(\"In run_copy_flag = no flow\") \r\n                \r\n                select_expr=\"\"\"\r\n                                    SELECT \r\n                                        c.cell_location,\r\n                                        c.cell_id,\r\n                                        c.bandwidth_usage_total,\r\n                                        c.signal_strength_indicator_1,\r\n                                        c.dropped_calls_total,\r\n                                        CAST(NULL as STRING) AS device_id,\r\n                                        CAST(0 as DECIMAL(11,3)) data_usage_total,\r\n                                        CAST(NULL as STRING) as network_quality_indicator_1,\r\n                                        CAST(NULL as INT) as handover_attempts,\r\n                                        c.data_timestamp\r\n                                    FROM cell_performance_data c\r\n                        \"\"\"\r\n            else:\r\n                \r\n                logger.info(\"In run_copy_flag = yes flow\") \r\n                \r\n                select_expr=\"\"\"\r\n                                  SELECT \r\n                                        c.cell_location,\r\n                                        c.cell_id,\r\n                                        c.bandwidth_usage_total,\r\n                                        c.signal_strength_indicator_1,\r\n                                        c.dropped_calls_total,\r\n                                        CAST(NULL as STRING) AS device_id,\r\n                                        CAST(0 as DECIMAL(11,3)) data_usage_total,\r\n                                        CAST(NULL as STRING) as network_quality_indicator_1,\r\n                                        CAST(-2147400000 as INT) as handover_attempts,\r\n                                        c.data_timestamp\r\n                                    FROM cell_performance_data c\r\n                        \"\"\"\r\n                \r\n                \r\n                \r\n        trf_df = spark.sql(select_expr)\r\n        \r\n        if log_level == 'DEBUG' :\r\n            trf_df.printSchema()\r\n            trf_df.show(2,False)\r\n        \r\n        parti_flag_new='_'.join(parti_ip.rsplit('/', 2))\r\n        \r\n        logger.info(f\"parti_flag=<{parti_flag_new}>\")\r\n\r\n        output_df = trf_df.sort(\"cell_id\",\"data_timestamp\")\r\n        \r\n        if output_num_files<=0:\r\n            logger.info(\"Inside output_num_files<=0\")\r\n            output_df = output_df\r\n        else:\r\n            logger.info(\"Inside output_num_files>0\")\r\n            output_df = output_df.repartition(output_num_files)\r\n        \r\n        output_path_save = output_path[:-1] + parti_ip.replace(source_table_path,target_table).replace('dbo','stage')\r\n        logger.info(f\"output_path_save=<{output_path_save}>\")\r\n        \r\n        \r\n        output_df.write.format(output_format).mode(\"overwrite\").save(output_path_save)\r\n    \r\n        output_path_flag=\"s3://\"+target_bucket + parti_flag_new\r\n        logger.info(f\"output_path_flag=<{output_path_flag}>\")\r\n        \r\n        create_flag_file(output_path_flag)\r\n        \r\n        parti_end_date = datetime.now().strftime('%Y%m%d%H%M%S')\r\n        \r\n\r\n        src_count_tbl_canbus=df.count()\r\n        \r\n        src_count=f\"{src_count_tbl_canbus}|{src_count_ue_statistics_data}\"\r\n        tgt_count_canbus_message=output_df.count()\r\n        parti_path=parti_flag_new[1:parti_flag_new.rfind('/')]\r\n        parti_name=parti_flag_new[parti_flag_new.rfind('/')+1:]\r\n        \r\n        logger.info(\"****************************************************************************************************************************************************************************\")\r\n        logger.info(f\"STATS_INFO=<TRANSFORMED,{source_table},{target_table},{parti_path},{parti_name},{parti_start_date},{parti_end_date},{src_count_tbl_canbus},{src_count_ue_statistics_data},{tgt_count_canbus_message}>\")\r\n        logger.info(\"****************************************************************************************************************************************************************************\")\r\n \r\n    \r\n        df_report = pd.concat([df_report, pd.DataFrame.from_records([{'PROCESS_NAME':'TRANSFORMED', 'SRC_TABLE_NAME':source_table, 'TGT_TABLE_NAME':target_table,'PARTITION_PATH':parti_path,'PARTITION_NAME':parti_name,'LOAD_START_TS':parti_start_date,'LOAD_END_TS':parti_end_date,'SRC_ROW_COUNT':src_count,'TGT_ROW_COUNT':int(tgt_count_canbus_message)}])])\r\n        \r\n        pd.set_option('display.max_rows', 100)\r\n        pd.set_option('display.max_columns', 20)\r\n        pd.set_option('display.width', 150)\r\n\r\n    if len(df_report) >= 1:\r\n        file_date = datetime.now().strftime('%Y%m%d%H%M%S')\r\n        report_output_path=parti_path.replace(source_table_path,target_table)\r\n\r\n        logger.info(f\"report_output_path=<{report_output_path}>\")\r\n        \r\n        file_path = f\"transform/{report_output_path}/{target_table}_stats_{file_date}.csv\"\r\n\r\n        csv_buffer = StringIO()\r\n        df_report.to_csv(csv_buffer)\r\n    \r\n        s3_resource = boto3.resource('s3')\r\n        s3_resource.Object(stats_data_bucket, file_path).put(Body=csv_buffer.getvalue())\r\n    else:\r\n        logger.warning(\"No rows to save!\")\r\n        \r\n\r\nexcept Exception as e:\r\n    print(f\"Glue Job <{job_nm}> FAILED. Exception: {str(e)}\")\r\n    \r\n    if len(df_report) >= 1:\r\n        file_date = datetime.now().strftime('%Y%m%d%H%M%S')\r\n        report_output_path=parti_path.replace(source_table_path,target_table)\r\n        \r\n        logger.info(f\"report_output_path=<{report_output_path}>\")\r\n        \r\n        file_path = f\"transform/{report_output_path}/{target_table}_stats_{file_date}.csv\" \r\n\r\n        csv_buffer = StringIO()\r\n        df_report.to_csv(csv_buffer)\r\n    \r\n        s3_resource = boto3.resource('s3')\r\n        s3_resource.Object(stats_data_bucket, file_path).put(Body=csv_buffer.getvalue())\r\n    else:\r\n        logger.warning(\"No rows to save!\")\r\n        \r\n    raise\r\n\r\n\r\n\r\njob.commit()  "
}